{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dying-texas",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../utils/dataset_building.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c7a77c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change it to True if you want to extract the sections from the 2017 dump in order to reproduce\n",
    "# results from 2017 paper by computing category section counts instead of using the provided ones\n",
    "reproducion_2017=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "funded-tribune",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import html\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "lasting-wisconsin",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tGetting all article ids...\n"
     ]
    }
   ],
   "source": [
    "all_articles=get_all_articles(\"../data/article_categories_filtered.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "organizational-charleston",
   "metadata": {},
   "outputs": [],
   "source": [
    "regex_sections = re.compile(r\"\\<h2\\>.+\\<\\/h2\\>\")\n",
    "regex_subsections=re.compile(r\"\\<h.\\>.+\\<\\/h.\\>\")\n",
    "regex_li_without_point=re.compile(r\"([^\\.])<\\/li\\>\")\n",
    "regex_dd_without_point=re.compile(r\"([^\\.])<\\/dd\\>\")\n",
    "regex_html_tags=re.compile(r\"\\<.+?\\>\")\n",
    "regex_multiple_spaces=re.compile(r\"\\s{2,}\")\n",
    "regex_wikipedia_template =re.compile(r\"\\{.+?\\}\")\n",
    "regex_remaining_links_replace=re.compile(r\"\\[\\[.+\\|(.+)\\]\\]\")\n",
    "# source: https://stackoverflow.com/a/3809435\n",
    "regex_http_links=re.compile(r\"https?:\\/\\/(www\\.)?[-a-zA-Z0-9@:%._\\+~#=]{1,256}\\.[a-zA-Z0-9()]{1,6}\\b([-a-zA-Z0-9()@:%_\\+.~#?&\\/=]*)\")\n",
    "regex_remaining_square_brackets=re.compile(r\"\\[\\[|\\]\\]\")\n",
    "regex_remaining_images=re.compile(r\"\\[\\[File:.+\\]\\]\")\n",
    "\n",
    "def clean_text(text):\n",
    "    # remove remaining html tags\n",
    "    cleaned_text=re.sub(regex_html_tags,\"\",text)\n",
    "    # remove remaining wikipedia templates, e.g. \"{{flagicon|Canada}} Canada\" => \"Canada\"\n",
    "    cleaned_text=re.sub(regex_wikipedia_template,\"\",cleaned_text)\n",
    "    # remove remaining wikipedia links in the form [[a|b]] and replace it with \"b\"\n",
    "    cleaned_text=re.sub(regex_remaining_links_replace,r\"\\1\",cleaned_text)\n",
    "    # remove http links\n",
    "    cleaned_text=re.sub(regex_http_links,\"\",cleaned_text)\n",
    "    # remove remaining square brackets\n",
    "    cleaned_text=re.sub(regex_remaining_square_brackets,\"\",cleaned_text)\n",
    "    # strangly, even with html.unescape, there were still some \"&nbsp;\" left, therefore we replace\n",
    "    # them with whitespaces\n",
    "    cleaned_text=cleaned_text.replace(\"&nbsp;\",\" \")\n",
    "    # replace multiple spaces inducted by lines where there was only a html tag without any text (e.g. <ul>)\n",
    "    cleaned_text=re.sub(regex_multiple_spaces,r\" \",cleaned_text)\n",
    "    \n",
    "    return cleaned_text\n",
    "\n",
    "def extract_section_content(text):\n",
    "    sections=re.findall(regex_sections,text)\n",
    "    \n",
    "    contents=re.split(regex_sections,text)\n",
    "    \n",
    "    section_contents=[]\n",
    "    # there is always a paragraph before the first section header, therefore we skip the first element of contents\n",
    "    for section,content in zip(sections,contents[1:]):\n",
    "        cleaned_section=section.replace(\"<h2>\",\"\").replace(\"</h2>\",\"\")\n",
    "        cleaned_section=clean_text(cleaned_section)\n",
    "        cleaned_section=cleaned_section.strip()\n",
    "        if len(cleaned_section)==0:\n",
    "            continue\n",
    "        \n",
    "        # in wikiextractor's output, the first line of each section is the name of the section with\n",
    "        # \"<h2>\" tags around, and the second line is the name of the section with a \".\" at the end,\n",
    "        # therefore we remove it\n",
    "        # additionnally, wikiextractor attempts to split sentences by separating them with newlines,\n",
    "        # we ignore it because we will split the text in sentences ourselves afterwards\n",
    "        cleaned_content=\" \".join(content.splitlines()[2:])\n",
    "        # remove all subsections with their hX tags, because these are duplicated without hX tags,\n",
    "        # e.g. \"<h3>Subsection</h3>\" is followed by \"Subsection\" on the line afterwards\n",
    "        cleaned_content=re.sub(regex_subsections,\"\",cleaned_content)\n",
    "        # if at the end of a list item, there is no \".\", add it\n",
    "        cleaned_content=re.sub(regex_li_without_point,r\"\\1.</li>\",cleaned_content)\n",
    "        cleaned_content=re.sub(regex_dd_without_point,r\"\\1.</dd>\",cleaned_content)\n",
    "        cleaned_content=re.sub(regex_remaining_images,\"\",cleaned_content)\n",
    "        \n",
    "        cleaned_content=clean_text(cleaned_content)\n",
    "\n",
    "        \n",
    "        section_contents.append({'section':cleaned_section,'content':cleaned_content})\n",
    "    \n",
    "    return section_contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "tropical-operations",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d0319ef70644aa4915fe3332208c12f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3643496 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# save article section contents\n",
    "pbar = tqdm(total = len(all_articles))\n",
    "with open(\"../data/article_section_contents.json\", \"a+\") as f:\n",
    "    # https://github.com/attardi/wikiextractor/blob/master/README.md\n",
    "    for line in subprocess.Popen(['wikiextractor', \"../data/enwiki-latest-pages-articles.xml.bz2\", '--templates', '../data/wikiextractor_templates', '-o', '-', '-q','--html','--html-safe','HTML_SAFE','--json','--processes','1'],  \n",
    "                                  stdout = subprocess.PIPE).stdout:\n",
    "        json_line=json.loads(line)\n",
    "        article_id=int(json_line['id'])\n",
    "        if article_id in all_articles:\n",
    "            article_text=json_line['text']\n",
    "            article_text=html.unescape(article_text)\n",
    "            section_contents=extract_section_content(article_text)\n",
    "            f.write(json.dumps({'article_id':article_id,'section_contents':section_contents})+\"\\n\")\n",
    "            pbar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "established-exhaust",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "769c49ea4edb4523a080aaf5fb1ecaf0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3643496 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# save article sections\n",
    "pbar = tqdm(total = len(all_articles))\n",
    "with open(\"../data/article_sections.json\", \"a+\") as f_out:\n",
    "    with open(\"../data/article_section_contents.json\", \"r\") as f_in:\n",
    "        for line in f_in:\n",
    "            json_line=json.loads(line)\n",
    "            article_id=json_line['article_id']\n",
    "            sections=[x['section'] for x in json_line['section_contents']]\n",
    "            f_out.write(json.dumps({'article_id':article_id,'sections':sections})+\"\\n\")\n",
    "            pbar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "printable-ordinance",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2656d9b8e2d4b98b1baa29bf9ee2f3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5132186 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# save article sections from 2017 dump for epfl reproduction\n",
    "if reproducion_2017:\n",
    "    all_articles_2017=get_all_articles(\"../data/epfl_paper/article_categories_sept17.tsv\")\n",
    "\n",
    "    pbar = tqdm(total = len(all_articles_2017))\n",
    "    with open(\"../data/article_sections_2017.json\", \"a+\") as f:\n",
    "        # https://github.com/attardi/wikiextractor/blob/master/README.md\n",
    "        for line in subprocess.Popen(['wikiextractor', \"../data/enwiki-20170820-pages-articles.xml.bz2\", '--templates', '../data/wikiextractor_templates_2017', '-o', '-', '-q','--html','--html-safe','HTML_SAFE','--json','--processes','1'],  \n",
    "                                      stdout = subprocess.PIPE).stdout:\n",
    "            json_line=json.loads(line)\n",
    "            article_id=int(json_line['id'])\n",
    "            if article_id in all_articles_2017:\n",
    "                article_text=json_line['text']\n",
    "                article_text=html.unescape(article_text)\n",
    "                section_contents=extract_section_content(article_text)\n",
    "                f.write(json.dumps({'article_id':article_id,'sections':[x['section'] for x in section_contents]})+\"\\n\")\n",
    "                pbar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "processed-preview",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit ('tm_venv')",
   "language": "python",
   "name": "python39564bittmvenv3226d3cd01364047aed0020f60ff15c7"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
