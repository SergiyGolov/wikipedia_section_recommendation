{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "sustainable-project",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../utils/dataset_building.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "latter-despite",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tGetting all article ids...\n"
     ]
    }
   ],
   "source": [
    "nb_articles=len(get_all_articles(\"../data/article_categories_no_unknown_types.tsv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "covered-oklahoma",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "romance-floor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_length: 256\n"
     ]
    }
   ],
   "source": [
    "max_length=SentenceTransformer('paraphrase-distilroberta-base-v2').max_seq_length\n",
    "print(f\"max_length: {max_length}\")\n",
    "min_length=20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aquatic-airfare",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "regex_multiple_spaces=re.compile(r\"\\s{2,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "sunset-hydrogen",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "token_count_sum_counter=Counter()\n",
    "def group_sentences(grouped_sentences,section,token_count_sum,section_grouped_sentences,section_total_token_count_dict):\n",
    "    global token_count_sum_counter\n",
    "    token_count_sum_counter[token_count_sum]+=1\n",
    "    if token_count_sum>min_length :\n",
    "        grouped_sentence=\" \".join(grouped_sentences)\n",
    "        grouped_sentence=re.sub(regex_multiple_spaces,r\" \",grouped_sentence)\n",
    "        section_grouped_sentences.append({'section':section,'sentence':grouped_sentence,'token_count':token_count_sum})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "returning-highland",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed56bbb21d074f7ab8272d001147a4da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2048191 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with open(\"../data/article_section_sentences_token_count.json\", \"r\") as f_in:\n",
    "    with open(\"../data/article_section_grouped_sentences.json\", \"a+\") as f_out:\n",
    "        for line in tqdm(f_in,total=nb_articles):\n",
    "            json_line=json.loads(line)\n",
    "            article_id=json_line['article_id']\n",
    "            section_sentences=json_line['section_sentences_token_count']\n",
    "            # key: section, value: list of sentences\n",
    "            section_sentences_dict={}\n",
    "            # key: section, value: number of tokens in this section\n",
    "            section_total_token_count_dict={}\n",
    "            for section_sentence in section_sentences:\n",
    "                section=section_sentence['section']\n",
    "                sentence=section_sentence['sentence']\n",
    "                token_count=section_sentence['token_count']\n",
    "                if section not in section_sentences_dict.keys():\n",
    "                    section_sentences_dict[section]=[]\n",
    "                    section_total_token_count_dict[section]=0\n",
    "                section_sentences_dict[section].append({'sentence':sentence,'token_count':token_count})\n",
    "                section_total_token_count_dict[section]+=token_count\n",
    "                \n",
    "            section_grouped_sentences=[]\n",
    "            for section,sentences_dict in section_sentences_dict.items():\n",
    "                token_count_sum=0\n",
    "                grouped_sentences=[]\n",
    "                for sentence_dict in sentences_dict:\n",
    "                    sentence=sentence_dict['sentence']\n",
    "                    token_count=sentence_dict['token_count']\n",
    "                    # we group sentences until the sum of their tokens counts is below max_length\n",
    "                    if token_count_sum+token_count<=max_length:\n",
    "                        token_count_sum+=token_count\n",
    "                        grouped_sentences.append(sentence)\n",
    "                    else:\n",
    "                        # once the token counts have reached the max_length limit, we group the sentences into a single\n",
    "                        group_sentences(grouped_sentences,section,token_count_sum,section_grouped_sentences,section_total_token_count_dict)\n",
    "                        grouped_sentences.clear()\n",
    "                        grouped_sentences.append(sentence)\n",
    "                        token_count_sum=token_count\n",
    "                if len(grouped_sentences)>0:\n",
    "                    group_sentences(grouped_sentences,section,token_count_sum,section_grouped_sentences,section_total_token_count_dict)\n",
    "            \n",
    "            if len(section_grouped_sentences)>0:\n",
    "                f_out.write(json.dumps({'article_id':article_id,'section_grouped_sentences':section_grouped_sentences})+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "sublime-tribute",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.04590493054744201"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# proportion of section contents that were removed because of the min_length\n",
    "sum([v for k,v in token_count_sum_counter.items() if k <min_length])/sum([v for v in token_count_sum_counter.values()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "gothic-receipt",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.001307963328792129"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# proportion of sentences which were already too long before the grouping,\n",
    "# those will be truncaned before being passed to the model\n",
    "sum([v for k,v in token_count_sum_counter.items() if k >max_length])/sum([v for v in token_count_sum_counter.values()])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit ('tm_venv')",
   "language": "python",
   "name": "python39564bittmvenv3226d3cd01364047aed0020f60ff15c7"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
