{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0331e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in this notebook, the word \"sentences\" is used instead of \"section contents\" like in the report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cordless-overhead",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../utils/common.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "right-westminster",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sentence_transformers.util import cos_sim\n",
    "from typing import List, Callable\n",
    "from collections import Counter\n",
    "import queue\n",
    "import networkx as nx\n",
    "from torch import Tensor\n",
    "import torch\n",
    "import pathlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "pleasant-joining",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(device):\n",
    "    return SentenceTransformer('paraphrase-distilroberta-base-v2',device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "collected-blast",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_to_embeddings(sentences,model):\n",
    "    return model.encode(sentences, show_progress_bar=False, batch_size=32, convert_to_tensor=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "empty-partnership",
   "metadata": {},
   "outputs": [],
   "source": [
    "#source: https://github.com/UKPLab/sentence-transformers/blob/10e1599339de3cefaedce91967275310c4c5dd82/sentence_transformers/util.py#L128\n",
    "# modified version, the goal is to ignore pairs of similar sentences which are from the same\n",
    "# section but in different articles or from the same article but in different sections\n",
    "def paraphrase_mining_embeddings(embeddings: Tensor,\n",
    "                      sentence_idx_section_map,\n",
    "                      sentence_idx_article_map,\n",
    "                      query_chunk_size: int = 5000,\n",
    "                      corpus_chunk_size: int = 100000,\n",
    "                      max_pairs: int = 500000,\n",
    "                      top_k: int = 100,\n",
    "                      score_function: Callable[[Tensor, Tensor], Tensor] = cos_sim):\n",
    "    \"\"\"\n",
    "    Given a list of sentences / texts, this function performs paraphrase mining. It compares all sentences against all\n",
    "    other sentences and returns a list with the pairs that have the highest cosine similarity score.\n",
    "    :param embeddings: A tensor with the embeddings\n",
    "    :param query_chunk_size: Search for most similar pairs for #query_chunk_size at the same time. Decrease, to lower memory footprint (increases run-time).\n",
    "    :param corpus_chunk_size: Compare a sentence simultaneously against #corpus_chunk_size other sentences. Decrease, to lower memory footprint (increases run-time).\n",
    "    :param max_pairs: Maximal number of text pairs returned.\n",
    "    :param top_k: For each sentence, we retrieve up to top_k other sentences\n",
    "    :param score_function: Function for computing scores. By default, cosine similarity.\n",
    "    :return: Returns a list of triplets with the format [score, id1, id2]\n",
    "    \"\"\"\n",
    "\n",
    "    top_k += 1  # A sentence has the highest similarity to itself. Increase +1 as we are interest in distinct pairs\n",
    "\n",
    "    # Mine for duplicates\n",
    "    pairs = queue.PriorityQueue()\n",
    "    min_score = 0.5\n",
    "    num_added = 0\n",
    "\n",
    "    for corpus_start_idx in range(0, len(embeddings), corpus_chunk_size):\n",
    "        for query_start_idx in range(0, len(embeddings), query_chunk_size):\n",
    "            scores = score_function(embeddings[query_start_idx:query_start_idx+query_chunk_size], embeddings[corpus_start_idx:corpus_start_idx+corpus_chunk_size])\n",
    "\n",
    "            scores_top_k_values, scores_top_k_idx = torch.topk(scores, min(top_k, len(scores[0])), dim=1, largest=True, sorted=False)\n",
    "            scores_top_k_values = scores_top_k_values.cpu().tolist()\n",
    "            scores_top_k_idx = scores_top_k_idx.cpu().tolist()\n",
    "\n",
    "            for query_itr in range(len(scores)):\n",
    "                for top_k_idx, corpus_itr in enumerate(scores_top_k_idx[query_itr]):\n",
    "                    i = query_start_idx + query_itr\n",
    "                    j = corpus_start_idx + corpus_itr\n",
    "\n",
    "                    if i != j and sentence_idx_section_map[i]!=sentence_idx_section_map[j] and sentence_idx_article_map[i]!=sentence_idx_article_map[j] and scores_top_k_values[query_itr][top_k_idx] > min_score:\n",
    "                        pairs.put((scores_top_k_values[query_itr][top_k_idx], i, j))\n",
    "                        num_added += 1\n",
    "\n",
    "                        if num_added >= max_pairs:\n",
    "                            entry = pairs.get()\n",
    "                            min_score = entry[0]\n",
    "\n",
    "    # Get the pairs\n",
    "    added_pairs = set()  # Used for duplicate detection\n",
    "    pairs_list = []\n",
    "    while not pairs.empty():\n",
    "        score, i, j = pairs.get()\n",
    "        sorted_i, sorted_j = sorted([i, j])\n",
    "\n",
    "        if sorted_i != sorted_j and (sorted_i, sorted_j) not in added_pairs:\n",
    "            added_pairs.add((sorted_i, sorted_j))\n",
    "            pairs_list.append([score, i, j])\n",
    "\n",
    "    # Highest scores first\n",
    "    pairs_list = sorted(pairs_list, key=lambda x: x[0], reverse=True)\n",
    "    return pairs_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "pleased-heather",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the goal is to scan the file once for multiple communities, we cannot fit all the section contents at same time\n",
    "def get_article_section_sentences_for_communities(community_sections,community_articles):\n",
    "    articles_to_get=set().union(*community_articles.values())\n",
    "    sections_to_get=set().union(*community_sections.values())\n",
    "    \n",
    "    article_section_sentences={}\n",
    "    with open(\"../data/article_section_grouped_sentences.json\", \"r\") as f_in:\n",
    "        for line in f_in:\n",
    "            json_line=json.loads(line)\n",
    "            article_id=json_line['article_id']\n",
    "            if article_id not in articles_to_get:\n",
    "                continue\n",
    "            section_sentences=[section_sentence for section_sentence in json_line['section_grouped_sentences'] if section_sentence['section'] in sections_to_get]\n",
    "            \n",
    "            already_added_sections=set()\n",
    "            section_first_sentences=[]\n",
    "            for section_sentence in section_sentences:\n",
    "                del section_sentence['token_count']\n",
    "                # we use only first grouped section content for each section\n",
    "                if section_sentence['section'] not in already_added_sections:\n",
    "                    section_first_sentences.append(section_sentence)\n",
    "                already_added_sections.add(section_sentence['section'])\n",
    "\n",
    "            article_section_sentences[article_id]=section_first_sentences\n",
    "            \n",
    "    return article_section_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "obvious-grove",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1128 23:34:39.012623 20776 SentenceTransformer.py:41] Load pretrained SentenceTransformer: paraphrase-distilroberta-base-v2\n",
      "I1128 23:34:39.016591 20776 SentenceTransformer.py:45] Did not find folder paraphrase-distilroberta-base-v2\n",
      "I1128 23:34:39.017087 20776 SentenceTransformer.py:51] Search model on server: http://sbert.net/models/paraphrase-distilroberta-base-v2.zip\n",
      "I1128 23:34:39.018079 20776 SentenceTransformer.py:107] Load SentenceTransformer from folder: C:\\Users\\Sergiy/.cache\\torch\\sentence_transformers\\sbert.net_models_paraphrase-distilroberta-base-v2\n"
     ]
    }
   ],
   "source": [
    "model=create_model(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "worse-label",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is useful if the notebook is stopped and restarted\n",
    "processed_communities=get_processed_communities()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "equal-telephone",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37e74cedae354694a6f6bcdde436de8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/24694 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n"
     ]
    }
   ],
   "source": [
    "pbar=tqdm(total=24694)\n",
    "batch_id=0\n",
    "with open(\"../data/community_article_and_sections_grouped_in_batches.json\", \"r\") as f_in:\n",
    "    for line in f_in:\n",
    "        batch_id+=1\n",
    "        \n",
    "        print(f\"batch {batch_id}\")\n",
    "        \n",
    "        json_line=json.loads(line)\n",
    "        community_sections={}\n",
    "        community_articles={}\n",
    "        \n",
    "        for x in json_line['community_articles_grouped_by_batch']:\n",
    "            community_articles[x['community_id']]=set(x['articles'])\n",
    "        for x in json_line['community_sections_grouped_by_batch']:\n",
    "            community_sections[x['community_id']]=set(x['sections'])\n",
    "        \n",
    "        article_section_sentences=get_article_section_sentences_for_communities(community_sections,community_articles)\n",
    "        \n",
    "        for community_id in community_sections.keys():\n",
    "            if community_id in processed_communities:\n",
    "                pbar.update(1)\n",
    "                continue\n",
    "            \n",
    "            sentences=[]\n",
    "            idx=0\n",
    "            # key: idx of sentence in \"sentences\" list, value: section from which this sentence is\n",
    "            sentence_idx_section_map={}\n",
    "            # key: idx of sentence in \"sentences\" list, value: article_id from which this sentence is\n",
    "            sentence_idx_article_map={}\n",
    "            for article_id in community_articles[community_id]:\n",
    "                if article_id in article_section_sentences.keys():\n",
    "                    for section_sentence in article_section_sentences[article_id]:\n",
    "                        section=section_sentence['section']\n",
    "                        if section in community_sections[community_id]:\n",
    "                            sentences.append(section_sentence['sentence'])\n",
    "                            sentence_idx_section_map[idx]=section\n",
    "                            sentence_idx_article_map[idx]=article_id\n",
    "                            idx+=1\n",
    "                        \n",
    "            sentence_counter_by_section=Counter(sentence_idx_section_map.values())\n",
    "            \n",
    "            if len(sentences)==0:\n",
    "                pbar.update(1)\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                embeddings=transform_to_embeddings(sentences,model)\n",
    "                paraphrases =paraphrase_mining_embeddings(embeddings,sentence_idx_section_map,sentence_idx_article_map)\n",
    "            # when CUDA runs out of memory, this exception occurs, this happened only before we limited\n",
    "            # the number of unique section contents inside each community, I'll keep it here if somebody's GPU\n",
    "            # will have not enough memory, in this case rerun notebooks 2 and 3 after decreasing max_nb_sentences\n",
    "            # in notebook 2\n",
    "            except RuntimeError as e:\n",
    "                print(e)\n",
    "                model = create_model(\"cpu\")\n",
    "                embeddings=transform_to_embeddings(sentences,model)\n",
    "                paraphrases =paraphrase_mining_embeddings(embeddings,sentence_idx_section_map,sentence_idx_article_map)\n",
    "                model = create_model(\"cuda\")\n",
    "            \n",
    "            if len(paraphrases)==0:\n",
    "                pbar.update(1)\n",
    "                continue\n",
    "            \n",
    "            pathlib.Path(f\"../data/semantic_similarity/community_{community_id}\").mkdir(parents=True, exist_ok=True) \n",
    "            \n",
    "            section_id_map={}\n",
    "            with open(f\"../data/semantic_similarity/community_{community_id}/sentence_counter_by_section.json\", \"a+\") as f_out:\n",
    "                for i,(section, nb_sentences) in enumerate(sentence_counter_by_section.most_common()):\n",
    "                    section_id_map[section]=i\n",
    "                    f_out.write(json.dumps({'section':section,'id':i,'nb_sentences':nb_sentences})+\"\\n\")\n",
    "            \n",
    "            section_pair_scores_dict={}\n",
    "            for paraphrase in paraphrases:\n",
    "\n",
    "                score, i, j = paraphrase\n",
    "                \n",
    "                section_A=section_id_map[sentence_idx_section_map[i]]\n",
    "                section_B=section_id_map[sentence_idx_section_map[j]]\n",
    "\n",
    "                sorted_pair=tuple(sorted((section_A,section_B)))\n",
    "\n",
    "                if sorted_pair not in section_pair_scores_dict.keys():\n",
    "                    section_pair_scores_dict[sorted_pair]=[]\n",
    "\n",
    "                section_pair_scores_dict[sorted_pair].append(score)\n",
    "                \n",
    "            with open(f\"../data/semantic_similarity/community_{community_id}/similar_section_pairs.json\", \"a+\") as f_out:\n",
    "                for (section_A_idx,section_B_idx),scores in section_pair_scores_dict.items():\n",
    "                    mean_score=sum(scores)/len(scores)\n",
    "                    nb_similar_sentences=len(scores)\n",
    "                    f_out.write(json.dumps({'section_A':section_A_idx,'section_B':section_B_idx,'mean_score':mean_score,'nb_similar_sentences':nb_similar_sentences})+\"\\n\")\n",
    "            \n",
    "            with open(f\"../data/semantic_similarity/processed_communities\", \"a+\") as f_out:\n",
    "                f_out.write(str(community_id)+\"\\n\")\n",
    "            \n",
    "            pbar.update(1)\n",
    "        \n",
    "        \n",
    "                        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
